{"text": "<a href=https://github.com/HFTrader/DeepLearningBook>[0] Bengio, Yoshua, Ian J. Goodfellow, and Aaron Courville. <b>'Deep learning.'</b> An MIT Press book. (2015).[[pdf]](https://github.com/HFTrader/DeepLearningBook) (Deep Learning Bible, you can read this book while reading following papers.) </a>"};
{"text": "<a href=http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf>[1] LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. <b>'Deep learning.'</b> Nature 521.7553 (2015): 436-444. [[pdf]](http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf) (Three Giants' Survey) </a>"};
{"text": "<a href=http://www.cs.toronto.edu/~hinton/absps/ncfast.pdf>[2] Hinton, Geoffrey E., Simon Osindero, and Yee-Whye Teh. <b>'A fast learning algorithm for deep belief nets.'</b> Neural computation 18.7 (2006): 1527-1554.[[pdf]](http://www.cs.toronto.edu/~hinton/absps/ncfast.pdf)(Deep Learning Eve) </a>"};
{"text": "<a href=http://www.cs.toronto.edu/~hinton/science.pdf>[3] Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. <b>'Reducing the dimensionality of data with neural networks.'</b> Science 313.5786 (2006): 504-507. [[pdf]](http://www.cs.toronto.edu/~hinton/science.pdf) (Milestone, Show the promise of deep learning) </a>"};
{"text": "<a href=http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf>[4] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. <b>'Imagenet classification with deep convolutional neural networks.'</b> Advances in neural information processing systems. 2012. [[pdf]](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) (AlexNet, Deep Learning Breakthrough) </a>"};
{"text": "<a href=https://arxiv.org/pdf/1409.1556.pdf>[5] Simonyan, Karen, and Andrew Zisserman. <b>'Very deep convolutional networks for large-scale image recognition.'</b> arXiv preprint arXiv:1409.1556 (2014).[[pdf]](https://arxiv.org/pdf/1409.1556.pdf) (VGGNet,Neural Networks become very deep!) </a>"};
{"text": "<a href=http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf>[6] Szegedy, Christian, et al. <b>'Going deeper with convolutions.'</b> Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015.[[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf) (GoogLeNet) </a>"};
{"text": "<a href=https://arxiv.org/pdf/1512.03385.pdf>[7] He, Kaiming, et al. <b>'Deep residual learning for image recognition.'</b> arXiv preprint arXiv:1512.03385 (2015).[[pdf]](https://arxiv.org/pdf/1512.03385.pdf) (ResNet,Very very deep networks, CVPR best paper) </a>"};
{"text": "<a href=http://cs224d.stanford.edu/papers/maas_paper.pdf>[8] Hinton, Geoffrey, et al. <b>'Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups.'</b> IEEE Signal Processing Magazine 29.6 (2012): 82-97.[[pdf]](http://cs224d.stanford.edu/papers/maas_paper.pdf) (Breakthrough in speech recognition)</a>"};
{"text": "<a href=http://arxiv.org/pdf/1303.5778.pdf>[9] Graves, Alex, Abdel-rahman Mohamed, and Geoffrey Hinton. <b>'Speech recognition with deep recurrent neural networks.'</b> 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013. [[pdf]](http://arxiv.org/pdf/1303.5778.pdf) (RNN)</a>"};
{"text": "<a href=http://www.jmlr.org/proceedings/papers/v32/graves14.pdf>[10] Graves, Alex, and Navdeep Jaitly. <b>'Towards End-To-End Speech Recognition with Recurrent Neural Networks.'</b> ICML. Vol. 14. 2014.[[pdf]](http://www.jmlr.org/proceedings/papers/v32/graves14.pdf)</a>"};
{"text": "<a href=http://arxiv.org/pdf/1507.06947>[11] Sak, Haşim, et al. <b>'Fast and accurate recurrent neural network acoustic models for speech recognition.'</b> arXiv preprint arXiv:1507.06947 (2015).[[pdf]](http://arxiv.org/pdf/1507.06947) (Google Speech Recognition System) </a>"};
{"text": "<a href=https://arxiv.org/pdf/1512.02595.pdf>[12] Amodei, Dario, et al. <b>'Deep speech 2: End-to-end speech recognition in english and mandarin.'</b> arXiv preprint arXiv:1512.02595 (2015).[[pdf]](https://arxiv.org/pdf/1512.02595.pdf) (Baidu Speech Recognition System) </a>"};
{"text": "<a href=https://arxiv.org/pdf/1610.05256v1>[13] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, G. Zweig <b>'Achieving Human Parity in Conversational Speech Recognition.'</b> arXiv preprint arXiv:1610.05256 (2016).[[pdf]](https://arxiv.org/pdf/1610.05256v1) (State-of-the-art in speech recognition, Microsoft) </a>"};
{"text": "<a href=https://arxiv.org/pdf/1207.0580.pdf>[14] Hinton, Geoffrey E., et al. <b>'Improving neural networks by preventing co-adaptation of feature detectors.'</b> arXiv preprint arXiv:1207.0580 (2012).[[pdf]](https://arxiv.org/pdf/1207.0580.pdf) (Dropout) </a>"};
{"text": "<a href=http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf>[15] Srivastava, Nitish, et al. <b>'Dropout: a simple way to prevent neural networks from overfitting.'</b> Journal of Machine Learning Research 15.1 (2014): 1929-1958.[[pdf]](http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1502.03167>[16] Ioffe, Sergey, and Christian Szegedy. <b>'Batch normalization: Accelerating deep network training by reducing internal covariate shift.'</b> arXiv preprint arXiv:1502.03167 (2015).[[pdf]](http://arxiv.org/pdf/1502.03167) (An outstanding Work in 2015) </a>"};
{"text": "<a href=https://arxiv.org/pdf/1607.06450.pdf?utm_source=sciontist.com&utm_medium=refer&utm_campaign=promote>[17] Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. <b>'Layer normalization.'</b> arXiv preprint arXiv:1607.06450 (2016).[[pdf]](https://arxiv.org/pdf/1607.06450.pdf?utm_source=sciontist.com&utm_medium=refer&utm_campaign=promote) (Update of Batch Normalization) </a>"};
{"text": "<a href=https://pdfs.semanticscholar.org/f832/b16cb367802609d91d400085eb87d630212a.pdf>[18] Courbariaux, Matthieu, et al. <b>'Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to+ 1 or−1.'</b> [[pdf]](https://pdfs.semanticscholar.org/f832/b16cb367802609d91d400085eb87d630212a.pdf) (New Model,Fast)  </a>"};
{"text": "<a href=https://arxiv.org/pdf/1608.05343>[19] Jaderberg, Max, et al. <b>'Decoupled neural interfaces using synthetic gradients.'</b> arXiv preprint arXiv:1608.05343 (2016). [[pdf]](https://arxiv.org/pdf/1608.05343) (Innovation of Training Method,Amazing Work) </a>"};
{"text": "<a href=http://www.jmlr.org/proceedings/papers/v28/sutskever13.pdf>[20] Sutskever, Ilya, et al. <b>'On the importance of initialization and momentum in deep learning.'</b> ICML (3) 28 (2013): 1139-1147.[[pdf]](http://www.jmlr.org/proceedings/papers/v28/sutskever13.pdf) (Momentum optimizer) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1412.6980>[21] Kingma, Diederik, and Jimmy Ba. <b>'Adam: A method for stochastic optimization.'</b> arXiv preprint arXiv:1412.6980 (2014).[[pdf]](http://arxiv.org/pdf/1412.6980) (Maybe used most often currently) </a>"};
{"text": "<a href=https://arxiv.org/pdf/1606.04474>[22] Andrychowicz, Marcin, et al. <b>'Learning to learn by gradient descent by gradient descent.'</b> arXiv preprint arXiv:1606.04474 (2016).[[pdf]](https://arxiv.org/pdf/1606.04474) (Neural Optimizer,Amazing Work) </a>"};
{"text": "<a href=https://pdfs.semanticscholar.org/5b6c/9dda1d88095fa4aac1507348e498a1f2e863.pdf>[23] Han, Song, Huizi Mao, and William J. Dally. <b>'Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding.'</b> CoRR, abs/1510.00149 2 (2015).[[pdf]](https://pdfs.semanticscholar.org/5b6c/9dda1d88095fa4aac1507348e498a1f2e863.pdf) (ICLR best paper, new direction to make NN running fast,DeePhi Tech Startup) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1602.07360>[24] Iandola, Forrest N., et al. <b>'SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 1MB model size.'</b> arXiv preprint arXiv:1602.07360 (2016). [[pdf]](http://arxiv.org/pdf/1602.07360) (Also a new direction to optimize NN,DeePhi Tech Startup) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1112.6209.pdf&embed>[25] Le, Quoc V. <b>'Building high-level features using large scale unsupervised learning.'</b> 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013.[[pdf]](http://arxiv.org/pdf/1112.6209.pdf&embed) (Milestone, Andrew Ng, Google Brain Project, Cat) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1312.6114>[26] Kingma, Diederik P., and Max Welling. <b>'Auto-encoding variational bayes.'</b> arXiv preprint arXiv:1312.6114 (2013).[[pdf]](http://arxiv.org/pdf/1312.6114) (VAE) </a>"};
{"text": "<a href=http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf>[27] Goodfellow, Ian, et al. <b>'Generative adversarial nets.'</b> Advances in Neural Information Processing Systems. 2014. [[pdf]](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) (GAN,super cool idea) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1511.06434>[28] Radford, Alec, Luke Metz, and Soumith Chintala. <b>'Unsupervised representation learning with deep convolutional generative adversarial networks.'</b> arXiv preprint arXiv:1511.06434 (2015). [[pdf]](http://arxiv.org/pdf/1511.06434) (DCGAN) </a>"};
{"text": "<a href=http://jmlr.org/proceedings/papers/v37/gregor15.pdf>[29] Gregor, Karol, et al. <b>'DRAW: A recurrent neural network for image generation.'</b> arXiv preprint arXiv:1502.04623 (2015). [[pdf]](http://jmlr.org/proceedings/papers/v37/gregor15.pdf) (VAE with attention, outstanding work) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1601.06759>[29] Oord, Aaron van den, Nal Kalchbrenner, and Koray Kavukcuoglu. <b>'Pixel recurrent neural networks.'</b> arXiv preprint arXiv:1601.06759 (2016). [[pdf]](http://arxiv.org/pdf/1601.06759) (PixelRNN) </a>"};
{"text": "<a href=https://arxiv.org/pdf/1606.05328>[30] Oord, Aaron van den, et al. <b>'Conditional image generation with PixelCNN decoders.'</b> arXiv preprint arXiv:1606.05328 (2016). [[pdf]](https://arxiv.org/pdf/1606.05328) (PixelCNN) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1308.0850>[31] Graves, Alex. <b>'Generating sequences with recurrent neural networks.'</b> arXiv preprint arXiv:1308.0850 (2013). [[pdf]](http://arxiv.org/pdf/1308.0850) (LSTM, very nice generating result, show the power of RNN) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1406.1078>[32] Cho, Kyunghyun, et al. <b>'Learning phrase representations using RNN encoder-decoder for statistical machine translation.'</b> arXiv preprint arXiv:1406.1078 (2014). [[pdf]](http://arxiv.org/pdf/1406.1078) (First Seq-to-Seq Paper) </a>"};
{"text": "<a href=http://papers.nips.cc/paper/5346-information-based-learning-by-agents-in-unbounded-state-spaces.pdf>[33] Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. <b>'Sequence to sequence learning with neural networks.'</b> Advances in neural information processing systems. 2014. [[pdf]](http://papers.nips.cc/paper/5346-information-based-learning-by-agents-in-unbounded-state-spaces.pdf) (Outstanding Work) </a>"};
{"text": "<a href=https://arxiv.org/pdf/1409.0473v7.pdf>[34] Bahdanau, Dzmitry, KyungHyun Cho, and Yoshua Bengio. <b>'Neural Machine Translation by Jointly Learning to Align and Translate.'</b> arXiv preprint arXiv:1409.0473 (2014). [[pdf]](https://arxiv.org/pdf/1409.0473v7.pdf) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1506.05869.pdf%20(http://arxiv.org/pdf/1506.05869.pdf>[35] Vinyals, Oriol, and Quoc Le. <b>'A neural conversational model.'</b> arXiv preprint arXiv:1506.05869 (2015). [[pdf]](http://arxiv.org/pdf/1506.05869.pdf%20(http://arxiv.org/pdf/1506.05869.pdf)) (Seq-to-Seq on Chatbot) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1410.5401.pdf>[36] Graves, Alex, Greg Wayne, and Ivo Danihelka. <b>'Neural turing machines.'</b> arXiv preprint arXiv:1410.5401 (2014). [[pdf]](http://arxiv.org/pdf/1410.5401.pdf) (Basic Prototype of Future Computer) </a>"};
{"text": "<a href=https://pdfs.semanticscholar.org/f10e/071292d593fef939e6ef4a59baf0bb3a6c2b.pdf>[37] Zaremba, Wojciech, and Ilya Sutskever. <b>'Reinforcement learning neural Turing machines.'</b> arXiv preprint arXiv:1505.00521 362 (2015). [[pdf]](https://pdfs.semanticscholar.org/f10e/071292d593fef939e6ef4a59baf0bb3a6c2b.pdf) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1410.3916>[38] Weston, Jason, Sumit Chopra, and Antoine Bordes. <b>'Memory networks.'</b> arXiv preprint arXiv:1410.3916 (2014).[[pdf]](http://arxiv.org/pdf/1410.3916) </a>"};
{"text": "<a href=http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf>[39] Sukhbaatar, Sainbayar, Jason Weston, and Rob Fergus. <b>'End-to-end memory networks.'</b> Advances in neural information processing systems. 2015. [[pdf]](http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf) </a>"};
{"text": "<a href=http://papers.nips.cc/paper/5866-pointer-networks.pdf>[40] Vinyals, Oriol, Meire Fortunato, and Navdeep Jaitly. <b>'Pointer networks.'</b> Advances in Neural Information Processing Systems. 2015. [[pdf]](http://papers.nips.cc/paper/5866-pointer-networks.pdf) </a>"};
{"text": "<a href=https://www.dropbox.com/s/0a40xi702grx3dq/2016-graves.pdf>[41] Graves, Alex, et al. <b>'Hybrid computing using a neural network with dynamic external memory.'</b> Nature (2016). [[pdf]](https://www.dropbox.com/s/0a40xi702grx3dq/2016-graves.pdf) (Milestone,combine above papers' ideas) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1312.5602.pdf>[42] Mnih, Volodymyr, et al. <b>'Playing atari with deep reinforcement learning.'</b> arXiv preprint arXiv:1312.5602 (2013). [[pdf]](http://arxiv.org/pdf/1312.5602.pdf)) (First Paper named deep reinforcement learning) </a>"};
{"text": "<a href=http://www.davidqiu.com:8888/research/nature14236.pdf>[43] Mnih, Volodymyr, et al. <b>'Human-level control through deep reinforcement learning.'</b> Nature 518.7540 (2015): 529-533. [[pdf]](http://www.davidqiu.com:8888/research/nature14236.pdf) (Milestone) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1511.06581>[44] Wang, Ziyu, Nando de Freitas, and Marc Lanctot. <b>'Dueling network architectures for deep reinforcement learning.'</b> arXiv preprint arXiv:1511.06581 (2015). [[pdf]](http://arxiv.org/pdf/1511.06581) (ICLR best paper,great idea)  </a>"};
{"text": "<a href=http://arxiv.org/pdf/1602.01783>[45] Mnih, Volodymyr, et al. <b>'Asynchronous methods for deep reinforcement learning.'</b> arXiv preprint arXiv:1602.01783 (2016). [[pdf]](http://arxiv.org/pdf/1602.01783) (State-of-the-art method) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1509.02971>[46] Lillicrap, Timothy P., et al. <b>'Continuous control with deep reinforcement learning.'</b> arXiv preprint arXiv:1509.02971 (2015). [[pdf]](http://arxiv.org/pdf/1509.02971) (DDPG) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1603.00748>[47] Gu, Shixiang, et al. <b>'Continuous Deep Q-Learning with Model-based Acceleration.'</b> arXiv preprint arXiv:1603.00748 (2016). [[pdf]](http://arxiv.org/pdf/1603.00748) (NAF) </a>"};
{"text": "<a href=http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf>[48] Schulman, John, et al. <b>'Trust region policy optimization.'</b> CoRR, abs/1502.05477 (2015). [[pdf]](http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf) (TRPO) </a>"};
{"text": "<a href=http://willamette.edu/~levenick/cs448/goNature.pdf>[49] Silver, David, et al. <b>'Mastering the game of Go with deep neural networks and tree search.'</b> Nature 529.7587 (2016): 484-489. [[pdf]](http://willamette.edu/~levenick/cs448/goNature.pdf) (AlphaGo) </a>"};
{"text": "<a href=http://www.jmlr.org/proceedings/papers/v27/bengio12a/bengio12a.pdf>[50] Bengio, Yoshua. <b>'Deep Learning of Representations for Unsupervised and Transfer Learning.'</b> ICML Unsupervised and Transfer Learning 27 (2012): 17-36. [[pdf]](http://www.jmlr.org/proceedings/papers/v27/bengio12a/bengio12a.pdf) (A Tutorial) </a>"};
{"text": "<a href=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.696.7800&rep=rep1&type=pdf>[51] Silver, Daniel L., Qiang Yang, and Lianghao Li. <b>'Lifelong Machine Learning Systems: Beyond Learning Algorithms.'</b> AAAI Spring Symposium: Lifelong Machine Learning. 2013. [[pdf]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.696.7800&rep=rep1&type=pdf) (A brief discussion about lifelong learning)  </a>"};
{"text": "<a href=http://arxiv.org/pdf/1503.02531>[52] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. <b>'Distilling the knowledge in a neural network.'</b> arXiv preprint arXiv:1503.02531 (2015). [[pdf]](http://arxiv.org/pdf/1503.02531) (Godfather's Work) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1511.06295>[53] Rusu, Andrei A., et al. <b>'Policy distillation.'</b> arXiv preprint arXiv:1511.06295 (2015). [[pdf]](http://arxiv.org/pdf/1511.06295) (RL domain) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1511.06342>[54] Parisotto, Emilio, Jimmy Lei Ba, and Ruslan Salakhutdinov. <b>'Actor-mimic: Deep multitask and transfer reinforcement learning.'</b> arXiv preprint arXiv:1511.06342 (2015). [[pdf]](http://arxiv.org/pdf/1511.06342) (RL domain) </a>"};
{"text": "<a href=https://arxiv.org/pdf/1606.04671>[55] Rusu, Andrei A., et al. <b>'Progressive neural networks.'</b> arXiv preprint arXiv:1606.04671 (2016). [[pdf]](https://arxiv.org/pdf/1606.04671) (Outstanding Work, A novel idea) </a>"};
{"text": "<a href=http://clm.utexas.edu/compjclub/wp-content/uploads/2016/02/lake2015.pdf>[56] Lake, Brenden M., Ruslan Salakhutdinov, and Joshua B. Tenenbaum. <b>'Human-level concept learning through probabilistic program induction.'</b> Science 350.6266 (2015): 1332-1338. [[pdf]](http://clm.utexas.edu/compjclub/wp-content/uploads/2016/02/lake2015.pdf) (No Deep Learning,but worth reading) </a>"};
{"text": "<a href=http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf>[57] Koch, Gregory, Richard Zemel, and Ruslan Salakhutdinov. <b>'Siamese Neural Networks for One-shot Image Recognition.'</b>(2015) [[pdf]](http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1605.06065>[58] Santoro, Adam, et al. <b>'One-shot Learning with Memory-Augmented Neural Networks.'</b> arXiv preprint arXiv:1605.06065 (2016). [[pdf]](http://arxiv.org/pdf/1605.06065) (A basic step to one shot learning) </a>"};
{"text": "<a href=https://arxiv.org/pdf/1606.04080>[59] Vinyals, Oriol, et al. <b>'Matching Networks for One Shot Learning.'</b> arXiv preprint arXiv:1606.04080 (2016). [[pdf]](https://arxiv.org/pdf/1606.04080) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1606.02819>[60] Hariharan, Bharath, and Ross Girshick. <b>'Low-shot visual object recognition.'</b> arXiv preprint arXiv:1606.02819 (2016). [[pdf]](http://arxiv.org/pdf/1606.02819) (A step to large data) </a>"};
{"text": "<a href=https://www.hds.utc.fr/~bordesan/dokuwiki/lib/exe/fetch.php?id=en%3Apubli&cache=cache&media=en:bordes12aistats.pdf>[61] Antoine Bordes, et al. <b>'Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing.'</b> AISTATS(2012) [[pdf]](https://www.hds.utc.fr/~bordesan/dokuwiki/lib/exe/fetch.php?id=en%3Apubli&cache=cache&media=en:bordes12aistats.pdf) </a>"};
{"text": "<a href=http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf>[62] Mikolov, et al. <b>'Distributed representations of words and phrases and their compositionality.'</b> ANIPS(2013): 3111-3119 [[pdf]](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) (word2vec) </a>"};
{"text": "<a href=http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf>[63] Sutskever, et al. <b>'“Sequence to sequence learning with neural networks.'</b> ANIPS(2014) [[pdf]](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) </a>"};
{"text": "<a href=https://arxiv.org/abs/1506.07285>[64] Ankit Kumar, et al. <b>'“Ask Me Anything: Dynamic Memory Networks for Natural Language Processing.'</b> arXiv preprint arXiv:1506.07285(2015) [[pdf]](https://arxiv.org/abs/1506.07285) </a>"};
{"text": "<a href=https://arxiv.org/abs/1508.06615>[65] Yoon Kim, et al. <b>'Character-Aware Neural Language Models.'</b> NIPS(2015) arXiv preprint arXiv:1508.06615(2015) [[pdf]](https://arxiv.org/abs/1508.06615) </a>"};
{"text": "<a href=https://arxiv.org/abs/1502.05698>[66] Jason Weston, et al. <b>'Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks.'</b> arXiv preprint arXiv:1502.05698(2015) [[pdf]](https://arxiv.org/abs/1502.05698) (bAbI tasks) </a>"};
{"text": "<a href=https://arxiv.org/abs/1506.03340>[67] Karl Moritz Hermann, et al. <b>'Teaching Machines to Read and Comprehend.'</b> arXiv preprint arXiv:1506.03340(2015) [[pdf]](https://arxiv.org/abs/1506.03340) (CNN/DailyMail cloze style questions) </a>"};
{"text": "<a href=http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf>[68] Szegedy, Christian, Alexander Toshev, and Dumitru Erhan. <b>'Deep neural networks for object detection.'</b> Advances in Neural Information Processing Systems. 2013. [[pdf]](http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf) </a>"};
{"text": "<a href=http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf>[69] Girshick, Ross, et al. <b>'Rich feature hierarchies for accurate object detection and semantic segmentation.'</b> Proceedings of the IEEE conference on computer vision and pattern recognition. 2014. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf) (RCNN) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1406.4729>[70] He, Kaiming, et al. <b>'Spatial pyramid pooling in deep convolutional networks for visual recognition.'</b> European Conference on Computer Vision. Springer International Publishing, 2014. [[pdf]](http://arxiv.org/pdf/1406.4729) (SPPNet) </a>"};
{"text": "<a href=https://pdfs.semanticscholar.org/8f67/64a59f0d17081f2a2a9d06f4ed1cdea1a0ad.pdf>[71] Girshick, Ross. <b>'Fast r-cnn.'</b> Proceedings of the IEEE International Conference on Computer Vision. 2015. [[pdf]](https://pdfs.semanticscholar.org/8f67/64a59f0d17081f2a2a9d06f4ed1cdea1a0ad.pdf) </a>"};
{"text": "<a href=http://papers.nips.cc/paper/5638-analysis-of-variational-bayesian-latent-dirichlet-allocation-weaker-sparsity-than-map.pdf>[72] Ren, Shaoqing, et al. <b>'Faster R-CNN: Towards real-time object detection with region proposal networks.'</b> Advances in neural information processing systems. 2015. [[pdf]](http://papers.nips.cc/paper/5638-analysis-of-variational-bayesian-latent-dirichlet-allocation-weaker-sparsity-than-map.pdf) </a>"};
{"text": "<a href=http://homes.cs.washington.edu/~ali/papers/YOLO.pdf>[73] Redmon, Joseph, et al. <b>'You only look once: Unified, real-time object detection.'</b> arXiv preprint arXiv:1506.02640 (2015). [[pdf]](http://homes.cs.washington.edu/~ali/papers/YOLO.pdf) (YOLO,Oustanding Work, really practical) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1512.02325>[74] Liu, Wei, et al. <b>'SSD: Single Shot MultiBox Detector.'</b> arXiv preprint arXiv:1512.02325 (2015). [[pdf]](http://arxiv.org/pdf/1512.02325) </a>"};
{"text": "<a href=http://papers.nips.cc/paper/5192-learning-a-deep-compact-image-representation-for-visual-tracking.pdf>[75] Wang, Naiyan, and Dit-Yan Yeung. <b>'Learning a deep compact image representation for visual tracking.'</b> Advances in neural information processing systems. 2013. [[pdf]](http://papers.nips.cc/paper/5192-learning-a-deep-compact-image-representation-for-visual-tracking.pdf) (First Paper to do visual tracking using Deep Learning,DLT Tracker) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1501.04587>[76] Wang, Naiyan, et al. <b>'Transferring rich feature hierarchies for robust visual tracking.'</b> arXiv preprint arXiv:1501.04587 (2015). [[pdf]](http://arxiv.org/pdf/1501.04587) (SO-DLT) </a>"};
{"text": "<a href=http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Wang_Visual_Tracking_With_ICCV_2015_paper.pdf>[77] Wang, Lijun, et al. <b>'Visual tracking with fully convolutional networks.'</b> Proceedings of the IEEE International Conference on Computer Vision. 2015. [[pdf]](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Wang_Visual_Tracking_With_ICCV_2015_paper.pdf) (FCNT) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1604.01802>[78] Held, David, Sebastian Thrun, and Silvio Savarese. <b>'Learning to Track at 100 FPS with Deep Regression Networks.'</b> arXiv preprint arXiv:1604.01802 (2016). [[pdf]](http://arxiv.org/pdf/1604.01802) (GOTURN,Really fast as a deep learning method,but still far behind un-deep-learning methods) </a>"};
{"text": "<a href=https://arxiv.org/pdf/1606.09549>[79] Bertinetto, Luca, et al. <b>'Fully-Convolutional Siamese Networks for Object Tracking.'</b> arXiv preprint arXiv:1606.09549 (2016). [[pdf]](https://arxiv.org/pdf/1606.09549) (SiameseFC,New state-of-the-art for real-time object tracking) </a>"};
{"text": "<a href=http://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/C-COT_ECCV16.pdf>[80] Martin Danelljan, Andreas Robinson, Fahad Khan, Michael Felsberg. <b>'Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Tracking.'</b> ECCV (2016) [[pdf]](http://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/C-COT_ECCV16.pdf) (C-COT) </a>"};
{"text": "<a href=https://arxiv.org/pdf/1608.07242>[81] Nam, Hyeonseob, Mooyeol Baek, and Bohyung Han. <b>'Modeling and Propagating CNNs in a Tree Structure for Visual Tracking.'</b> arXiv preprint arXiv:1608.07242 (2016). [[pdf]](https://arxiv.org/pdf/1608.07242) (VOT2016 Winner,TCNN) </a>"};
{"text": "<a href=https://www.cs.cmu.edu/~afarhadi/papers/sentence.pdf>[82] Farhadi,Ali,etal. <b>'Every picture tells a story: Generating sentences from images'</b>. In Computer VisionECCV 2010. Springer Berlin Heidelberg:15-29, 2010. [[pdf]](https://www.cs.cmu.edu/~afarhadi/papers/sentence.pdf) </a>"};
{"text": "<a href=http://tamaraberg.com/papers/generation_cvpr11.pdf>[83] Kulkarni, Girish, et al. <b>'Baby talk: Understanding and generating image descriptions'</b>. In Proceedings of the 24th CVPR, 2011. [[pdf]](http://tamaraberg.com/papers/generation_cvpr11.pdf)</a>"};
{"text": "<a href=https://arxiv.org/pdf/1411.4555.pdf>[84] Vinyals, Oriol, et al. <b>'Show and tell: A neural image caption generator'</b>. In arXiv preprint arXiv:1411.4555, 2014.[[pdf]](https://arxiv.org/pdf/1411.4555.pdf)</a>"};
{"text": "<a href=https://arxiv.org/pdf/1411.4389.pdf>[85] Donahue, Jeff, et al. <b>'Long-term recurrent convolutional networks for visual recognition and description'</b>. In arXiv preprint arXiv:1411.4389 ,2014. [[pdf]](https://arxiv.org/pdf/1411.4389.pdf)</a>"};
{"text": "<a href=https://cs.stanford.edu/people/karpathy/cvpr2015.pdf>[86] Karpathy, Andrej, and Li Fei-Fei. <b>'Deep visual-semantic alignments for generating image descriptions'</b>. In arXiv preprint arXiv:1412.2306, 2014. [[pdf]](https://cs.stanford.edu/people/karpathy/cvpr2015.pdf)</a>"};
{"text": "<a href=https://arxiv.org/pdf/1406.5679v1.pdf>[87] Karpathy, Andrej, Armand Joulin, and Fei Fei F. Li. <b>'Deep fragment embeddings for bidirectional image sentence mapping'</b>. In Advances in neural information processing systems, 2014. [[pdf]](https://arxiv.org/pdf/1406.5679v1.pdf)</a>"};
{"text": "<a href=https://arxiv.org/pdf/1411.4952v3.pdf>[88] Fang, Hao, et al. <b>'From captions to visual concepts and back'</b>. In arXiv preprint arXiv:1411.4952, 2014. [[pdf]](https://arxiv.org/pdf/1411.4952v3.pdf)</a>"};
{"text": "<a href=https://arxiv.org/pdf/1411.5654v1.pdf>[89] Chen, Xinlei, and C. Lawrence Zitnick. <b>'Learning a recurrent visual representation for image caption generation'</b>. In arXiv preprint arXiv:1411.5654, 2014. [[pdf]](https://arxiv.org/pdf/1411.5654v1.pdf)</a>"};
{"text": "<a href=https://arxiv.org/pdf/1412.6632v5.pdf>[90] Mao, Junhua, et al. <b>'Deep captioning with multimodal recurrent neural networks (m-rnn)'</b>. In arXiv preprint arXiv:1412.6632, 2014.[[pdf]](https://arxiv.org/pdf/1412.6632v5.pdf)</a>"};
{"text": "<a href=https://arxiv.org/pdf/1502.03044v3.pdf>[91] Xu, Kelvin, et al. <b>'Show, attend and tell: Neural image caption generation with visual attention'</b>. In arXiv preprint arXiv:1502.03044, 2015. [[pdf]](https://arxiv.org/pdf/1502.03044v3.pdf)</a>"};
{"text": "<a href=http://arxiv.org/pdf/1410.8206>[92] Luong, Minh-Thang, et al. <b>'Addressing the rare word problem in neural machine translation.'</b> arXiv preprint arXiv:1410.8206 (2014).[[pdf]](http://arxiv.org/pdf/1410.8206) </a>"};
{"text": "<a href=https://arxiv.org/pdf/1508.07909.pdf>[93] Sennrich, et al. <b>'Neural Machine Translation of Rare Words with Subword Units'</b>. In arXiv preprint arXiv:1508.07909, 2015. [[pdf]](https://arxiv.org/pdf/1508.07909.pdf)</a>"};
{"text": "<a href=http://arxiv.org/pdf/1508.04025>[94] Luong, Minh-Thang, Hieu Pham, and Christopher D. Manning. <b>'Effective approaches to attention-based neural machine translation.'</b> arXiv preprint arXiv:1508.04025 (2015). [[pdf]](http://arxiv.org/pdf/1508.04025) </a>"};
{"text": "<a href=https://arxiv.org/pdf/1603.06147.pdf>[95] Chung, et al. <b>'A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation'</b>. In arXiv preprint arXiv:1603.06147, 2016. [[pdf]](https://arxiv.org/pdf/1603.06147.pdf)</a>"};
{"text": "<a href=https://arxiv.org/pdf/1610.03017.pdf>[96] Lee, et al. <b>'Fully Character-Level Neural Machine Translation without Explicit Segmentation'</b>. In arXiv preprint arXiv:1610.03017, 2016. [[pdf]](https://arxiv.org/pdf/1610.03017.pdf)</a>"};
{"text": "<a href=https://arxiv.org/pdf/1609.08144v2.pdf>[97] Wu, Schuster, Chen, Le, et al. <b>'Google'</b>s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation'. In arXiv preprint arXiv:1609.08144v2, 2016. [[pdf]](https://arxiv.org/pdf/1609.08144v2.pdf) (Milestone) </a>"};
{"text": "<a href=http://repository.supsi.ch/4550/1/koutnik2013gecco.pdf>[98] Koutník, Jan, et al. <b>'Evolving large-scale neural networks for vision-based reinforcement learning.'</b> Proceedings of the 15th annual conference on Genetic and evolutionary computation. ACM, 2013. [[pdf]](http://repository.supsi.ch/4550/1/koutnik2013gecco.pdf) </a>"};
{"text": "<a href=http://www.jmlr.org/papers/volume17/15-522/15-522.pdf>[99] Levine, Sergey, et al. <b>'End-to-end training of deep visuomotor policies.'</b> Journal of Machine Learning Research 17.39 (2016): 1-40. [[pdf]](http://www.jmlr.org/papers/volume17/15-522/15-522.pdf) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1509.06825>[100] Pinto, Lerrel, and Abhinav Gupta. <b>'Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours.'</b> arXiv preprint arXiv:1509.06825 (2015). [[pdf]](http://arxiv.org/pdf/1509.06825) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1603.02199>[101] Levine, Sergey, et al. <b>'Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection.'</b> arXiv preprint arXiv:1603.02199 (2016). [[pdf]](http://arxiv.org/pdf/1603.02199) </a>"};
{"text": "<a href=https://arxiv.org/pdf/1609.05143>[102] Zhu, Yuke, et al. <b>'Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning.'</b> arXiv preprint arXiv:1609.05143 (2016). [[pdf]](https://arxiv.org/pdf/1609.05143) </a>"};
{"text": "<a href=https://arxiv.org/pdf/1610.00673>[103] Yahya, Ali, et al. <b>'Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search.'</b> arXiv preprint arXiv:1610.00673 (2016).[[pdf]](https://arxiv.org/pdf/1610.00673) </a>"};
{"text": "<a href=https://arxiv.org/pdf/1610.00633>[104] Gu, Shixiang, et al. <b>'Deep Reinforcement Learning for Robotic Manipulation.'</b> arXiv preprint arXiv:1610.00633 (2016). [[pdf]](https://arxiv.org/pdf/1610.00633) </a>"};
{"text": "<a href=https://arxiv.org/pdf/1610.04286.pdf>[105] A Rusu, M Vecerik, Thomas Rothörl, N Heess, R Pascanu, R Hadsell.<b>'Sim-to-Real Robot Learning from Pixels with Progressive Nets.'</b> arXiv preprint arXiv:1610.04286 (2016). [[pdf]](https://arxiv.org/pdf/1610.04286.pdf) </a>"};
{"text": "<a href=https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html>[106] Mordvintsev, Alexander, Olah, Christopher, Tyka, Mike (2015). <b>'Inceptionism: Going Deeper into Neural Networks'</b>. Google Research. [[html]](https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html) (Deep Dream) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1508.06576>[107] Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. <b>'A neural algorithm of artistic style.'</b> arXiv preprint arXiv:1508.06576 (2015). [[pdf]](http://arxiv.org/pdf/1508.06576) (Outstanding Work, most successful method currently) </a>"};
{"text": "<a href=https://arxiv.org/pdf/1609.03552>[108] Zhu, Jun-Yan, et al. <b>'Generative Visual Manipulation on the Natural Image Manifold.'</b> European Conference on Computer Vision. Springer International Publishing, 2016. [[pdf]](https://arxiv.org/pdf/1609.03552) (iGAN) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1603.01768>[109] Champandard, Alex J. <b>'Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks.'</b> arXiv preprint arXiv:1603.01768 (2016). [[pdf]](http://arxiv.org/pdf/1603.01768) (Neural Doodle) </a>"};
{"text": "<a href=http://arxiv.org/pdf/1603.08511>[110] Zhang, Richard, Phillip Isola, and Alexei A. Efros. <b>'Colorful Image Colorization.'</b> arXiv preprint arXiv:1603.08511 (2016). [[pdf]](http://arxiv.org/pdf/1603.08511) </a>"};
{"text": "<a href=https://arxiv.org/pdf/1603.08155.pdf>[111] Johnson, Justin, Alexandre Alahi, and Li Fei-Fei. <b>'Perceptual losses for real-time style transfer and super-resolution.'</b> arXiv preprint arXiv:1603.08155 (2016). [[pdf]](https://arxiv.org/pdf/1603.08155.pdf) </a>"}